# RHFS90_MPI - MPI并行超精细结构计算程序

## 概述

RHFS90_MPI是基于GRASP原版RHFS90程序开发的MPI并行版本，用于计算原子的超精细结构参数。该程序实现了主要计算循环的并行化，能够显著提高大型原子体系计算的效率。

## 并行化策略

程序的主要并行化在以下方面：

1. **配置状态函数循环并行化**: 将配置状态函数的双重循环 (IC, IR) 在不同MPI进程间分发
2. **工作负载分配**: 每个进程处理 IC = myid+1, myid+1+nprocs, myid+1+2*nprocs, ... 的值
3. **结果汇总**: 使用 MPI_ALLREDUCE 将各进程的计算结果汇总
4. **文件输出**: 仅0号进程负责结果输出，避免文件冲突

## 程序结构

- `rhfs90mpi.f90`: MPI主程序，负责初始化、进程管理和程序流程控制
- `hfsggmpi.f90`: 并行化的超精细结构计算核心函数
- `hfsggmpi_I.f90`: 接口文件

## 编译和安装

### 使用CMake (推荐)

```bash
cd /path/to/rhfs_ris_mpi
mkdir build
cd build
cmake ..
make rhfs90mpi
```

### 使用构建脚本

```bash
cd src/appl/rhfs90_mpi
./BUILDCONF.sh
```

## 运行方法

### 使用mpirun

```bash
mpirun -np <进程数> rhfs90mpi
```

### 输入文件

程序需要以下输入文件：
- `isodata`: 同位素数据
- `<name>.c`: 配置状态函数文件  
- `<name>.cm` 或 `<name>.m`: 混合系数文件
- `<name>.w`: 波函数文件

### 输出文件

程序生成以下输出文件：
- `<name>.h` 或 `<name>.ch`: 超精细结构常数
- `<name>.hoffd`: 非对角元素文件

## 性能说明

- 推荐使用进程数不超过配置状态函数数量
- 对于大型计算，建议使用16-64个进程
- 内存需求与串行版本相当，但通过并行可提高计算速度

## 与原版差异

1. 增加了MPI并行支持
2. 修改了主计算循环的分配策略
3. 添加了进程间通信和结果汇总
4. 保持了与原版完全相同的计算精度

## 依赖库

- MPI库 (OpenMPI, MPICH等)
- GRASP库文件 (lib9290, libmod, mpi90)
- BLAS/LAPACK库

## 注意事项

1. 所有进程必须能访问相同的输入文件
2. 仅0号进程生成输出文件
3. 程序会自动检测可用进程数并分配工作
4. 建议在具有高速网络的集群上运行以获得最佳性能

## 错误处理

程序包含完整的MPI错误处理机制，在出现错误时会统一终止所有进程并给出错误信息。

## 版本信息

- 基于GRASP92版本
- MPI并行化版本创建于2024年12月
- 兼容原版RHFS90的所有功能
